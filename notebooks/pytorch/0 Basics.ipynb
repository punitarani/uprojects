{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "Load the `MNIST` dataset from TorchVision.\n",
    "- This dataset contains 60,000 training images and 10,000 test images of handwritten digits.\n",
    "- Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.\n",
    "- Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker.\n",
    "- This pixel-value is an integer between 0 and 255, inclusive.\n",
    "\n",
    "`datasets.MNIST()` accepts the following parameters:\n",
    "- `root` - the path where the train/test data is stored\n",
    "- `train` - a boolean value that indicates whether the dataset should be the training dataset or the test dataset\n",
    "- `download` - a boolean value that indicates whether the dataset should be downloaded if it's not available at `root`\n",
    "- `transform` - an optional transform function that takes in an PIL image and returns a transformed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`torch.utils.data.Dataset` is an abstract class representing a dataset. It stores the samples and their corresponding labels.\n",
    "`torch.utils.data.DataLoader` wraps an iterable around the `Dataset`.\n",
    "\n",
    "`DataLoader` supports both map-style and iterable-style datasets with single- or multi-process loading, customizing loading order and optional automatic batching (collation) and memory pinning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n",
      "Shape of y:  torch.Size([64]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(\"Shape of X [N, C, H, W]: \", X.shape)\n",
    "    print(\"Shape of y: \", y.shape, y.dtype)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Model\n",
    "\n",
    "### `device`\n",
    "\n",
    "`torch.device` is an object representing the device on which a `torch.Tensor` is or will be allocated.\n",
    "`torch.device` objects are used as arguments to many methods in the `torch` package.\n",
    "`torch.device` objects are typically constructed using `torch.device(\"cuda\")` or `torch.device(\"cpu\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using {} device\".format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `nn.Module`\n",
    "\n",
    "- `nn.Module` is the base class for all neural network modules.\n",
    "- The layers of the model are defined in the `__init__` function and the forward pass of the model is defined in the `forward` function.\n",
    "- `nn.Module` keeps track of the layers and parameters of the model.\n",
    "- The `device` argument is used to move the model to the GPU if available. The default value is `cpu`.\n",
    "\n",
    "#### Common Layers\n",
    "\n",
    "- `nn.Conv2d`: Convolutional layer for 2D inputs.\n",
    "- `nn.CrossEntropyLoss`: Computes the cross-entropy loss between the input and target.\n",
    "- `nn.Dropout`: Randomly zeroes some of the elements of the input tensor with a given probability during training.\n",
    "- `nn.Embedding`: Embedding layer that maps discrete inputs to continuous vectors.\n",
    "- `nn.Flatten`: Flattens a contiguous range of dims into a 1D tensor.\n",
    "- `nn.Linear`: Linear layer that applies a linear transformation to the input tensor.\n",
    "- `nn.LSTM`: Long Short-Term Memory recurrent neural network layer.\n",
    "- `nn.MaxPool2d`: Max pooling layer for 2D inputs.\n",
    "- `nn.ReLU`: Rectified Linear Unit activation function.\n",
    "- `nn.Softmax`: Softmax activation function that converts the input tensor to a probability distribution.\n",
    "\n",
    "> Read more about the supported layers [here](https://pytorch.org/docs/stable/nn.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "    (5): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a model instance and move it to the GPU if available\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "To train a model, we need to define the following:\n",
    "\n",
    "- **Loss function** - measures how well the output of a model for a given input matches the target output. The goal of training is to minimize this difference.\n",
    "- **Optimizer** - defines how the model parameters are updated based on the loss function. The simplest optimizer is stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print loss every 100 batches\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "We will test the model's accuracy on the test dataset to ensure it is learning.\n",
    "We will check the accuracy by comparing the model's predictions to the ground-truth labels of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Turn off gradients for validation, saves memory and computations (no need to compute gradients for validation)\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # Compute prediction error\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item() # sum up batch loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # sum up correct predictions\n",
    "\n",
    "    # Calculate average loss and accuracy\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, and Save the Model\n",
    "\n",
    "`epochs` is the number of times the model will iterate over the entire training dataset.\n",
    "In each iteration, the model learns the parameters to better map inputs to outputs.\n",
    "It is common to see the loss decrease and the accuracy increase with each epoch.\n",
    "\n",
    "> **Note**: Training can take a few minutes, depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.304379  [    0/60000]\n",
      "loss: 2.303697  [ 6400/60000]\n",
      "loss: 2.294250  [12800/60000]\n",
      "loss: 2.294914  [19200/60000]\n",
      "loss: 2.299555  [25600/60000]\n",
      "loss: 2.298127  [32000/60000]\n",
      "loss: 2.284647  [38400/60000]\n",
      "loss: 2.284154  [44800/60000]\n",
      "loss: 2.282887  [51200/60000]\n",
      "loss: 2.275116  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 29.2%, Avg loss: 2.281900 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 2.283860  [    0/60000]\n",
      "loss: 2.283749  [ 6400/60000]\n",
      "loss: 2.275879  [12800/60000]\n",
      "loss: 2.268411  [19200/60000]\n",
      "loss: 2.283331  [25600/60000]\n",
      "loss: 2.283383  [32000/60000]\n",
      "loss: 2.258339  [38400/60000]\n",
      "loss: 2.261001  [44800/60000]\n",
      "loss: 2.256265  [51200/60000]\n",
      "loss: 2.245658  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.0%, Avg loss: 2.255985 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.259030  [    0/60000]\n",
      "loss: 2.258939  [ 6400/60000]\n",
      "loss: 2.251599  [12800/60000]\n",
      "loss: 2.232405  [19200/60000]\n",
      "loss: 2.260883  [25600/60000]\n",
      "loss: 2.263092  [32000/60000]\n",
      "loss: 2.220415  [38400/60000]\n",
      "loss: 2.226692  [44800/60000]\n",
      "loss: 2.217083  [51200/60000]\n",
      "loss: 2.201033  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 32.8%, Avg loss: 2.217268 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 2.221024  [    0/60000]\n",
      "loss: 2.222201  [ 6400/60000]\n",
      "loss: 2.214737  [12800/60000]\n",
      "loss: 2.178121  [19200/60000]\n",
      "loss: 2.227829  [25600/60000]\n",
      "loss: 2.232346  [32000/60000]\n",
      "loss: 2.162054  [38400/60000]\n",
      "loss: 2.173328  [44800/60000]\n",
      "loss: 2.157639  [51200/60000]\n",
      "loss: 2.132218  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 33.4%, Avg loss: 2.158815 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 2.163538  [    0/60000]\n",
      "loss: 2.166996  [ 6400/60000]\n",
      "loss: 2.157920  [12800/60000]\n",
      "loss: 2.099996  [19200/60000]\n",
      "loss: 2.179489  [25600/60000]\n",
      "loss: 2.187356  [32000/60000]\n",
      "loss: 2.078169  [38400/60000]\n",
      "loss: 2.096563  [44800/60000]\n",
      "loss: 2.075066  [51200/60000]\n",
      "loss: 2.037012  [57600/60000]\n",
      "Test Error: \n",
      " Accuracy: 35.0%, Avg loss: 2.079046 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "The model's state dictionary contains the learned parameters.\n",
    "\n",
    "`torch.save` is used to save the model's state dictionary to a file.\n",
    "`.pth` is the common file extension for PyTorch models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Model State to basic-mnist.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"basic-mnist.pth\")\n",
    "print(\"Saved PyTorch Model State to basic-mnist.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model\n",
    "\n",
    "`torch.load` is used to load the model's state dictionary from a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PyTorch Model State from basic-mnist.pth\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork()\n",
    "model.load_state_dict(torch.load(\"basic-mnist.pth\"))\n",
    "print(\"Loaded PyTorch Model State from basic-mnist.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"7\", Actual: \"7\"\n"
     ]
    }
   ],
   "source": [
    "classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "\n",
    "model.eval()\n",
    "x, y = test_data[0][0], test_data[0][1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test with a random image from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: \"6\", Actual: \"5\"\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model.eval()\n",
    "rand_index = random.randint(0, len(test_data))\n",
    "x, y = test_data[rand_index][0], test_data[rand_index][1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(x)\n",
    "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
    "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')\n",
    "\n",
    "    plt.imshow(x.view(28, 28), cmap='gray')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
